{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1751af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f06f82-7eaf-4877-8aa4-874293a448f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_observations, num_hidden, num_actions):\n",
    "    observations = layers.Input(shape=(num_observations,))\n",
    "    hidden = layers.Dense(num_hidden, activation='relu')(observations)\n",
    "    action = layers.Dense(num_actions, activation='softmax')(hidden)\n",
    "    \n",
    "    return keras.Model(inputs=observations, outputs=action)\n",
    "    \n",
    "class Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_observations = 4\n",
    "        self.num_hidden = 128\n",
    "        self.num_actions = 2\n",
    "\n",
    "        self.batch_size = 128\n",
    "        self.max_memory_length = 10000\n",
    "\n",
    "        self.epsilon = 0.99\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.gamma = 0.7\n",
    "        \n",
    "        self.action_history = []\n",
    "        self.state_history = []\n",
    "        self.state_next_history = []\n",
    "        self.rewards_history = []\n",
    "        self.done_history = []\n",
    "        self.episode_reward_history = []\n",
    "        self.running_reward = 0\n",
    "        self.episode_count = 0\n",
    "        \n",
    "        self.loss_function = keras.losses.Huber()\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "    \n",
    "        self.policy_net = create_model(self.num_observations, self.num_hidden, self.num_actions)\n",
    "        self.target_net = create_model(self.num_observations, self.num_hidden, self.num_actions)\n",
    "\n",
    "    def take_action(self, state):\n",
    "        if self.epsilon > np.random.rand(1)[0]:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = self.policy_net(state_tensor, training=False)\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "    \n",
    "        state_next, reward, done, _, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "            \n",
    "        self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "        self.action_history.append(action)\n",
    "        self.state_history.append(state)\n",
    "        self.state_next_history.append(state_next)\n",
    "        self.done_history.append(done)\n",
    "        self.rewards_history.append(reward)\n",
    "        \n",
    "        state = state_next\n",
    "\n",
    "        return state, reward, done\n",
    "\n",
    "    def get_samples(self):\n",
    "        indices = np.random.choice(range(len(agent.done_history)), size=agent.batch_size)\n",
    "\n",
    "        state_sample = np.array([  self.state_history[i] for i in indices])\n",
    "        state_next_sample = np.array([  self.state_next_history[i] for i in indices])\n",
    "        rewards_sample = [  self.rewards_history[i] for i in indices]\n",
    "        action_sample = [  self.action_history[i] for i in indices]\n",
    "        done_sample = tf.convert_to_tensor(\n",
    "            [float(  self.done_history[i]) for i in indices]\n",
    "        )\n",
    "\n",
    "        return state_sample, state_next_sample, rewards_sample, action_sample, done_sample\n",
    "\n",
    "    def train_model(self, state_sample, state_next_sample, rewards_sample, action_sample, done_sample):\n",
    "        future_rewards = self.target_net.predict(state_next_sample)\n",
    "        updated_q_values = rewards_sample + self.gamma * tf.reduce_max(\n",
    "            future_rewards, axis=1\n",
    "        )\n",
    "        updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "        masks = tf.one_hot(action_sample, self.num_actions)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.policy_net(state_sample)\n",
    "            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "            loss = self.loss_function(updated_q_values, q_action)\n",
    "\n",
    "        grads = tape.gradient(loss, self.policy_net.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.policy_net.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed2e69a-aa12-41f6-bc9a-fb6108e38a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "\n",
    "for i in range(2000):\n",
    "    \n",
    "    state = np.array(env.reset()[0])\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(100):\n",
    "        state, reward, done = agent.take_action(state)\n",
    "        episode_reward += reward\n",
    "\n",
    "        if len(agent.done_history) > agent.batch_size:\n",
    "            state_sample, state_next_sample, rewards_sample, action_sample, done_sample = agent.get_samples()\n",
    "            agent.train_model(state_sample, state_next_sample, rewards_sample, action_sample, done_sample)\n",
    "            \n",
    "        if timestep % 10000 == 0:\n",
    "            agent.target_net.set_weights(agent.policy_net.get_weights())\n",
    "            template = \"running reward: {:.2f} at episode {}\"\n",
    "            print(template.format(agent.running_reward, agent.episode_count))\n",
    "\n",
    "        if len(agent.rewards_history) > agent.max_memory_length:\n",
    "            del agent.rewards_history[:1]\n",
    "            del agent.state_history[:1]\n",
    "            del agent.state_next_history[:1]\n",
    "            del agent.action_history[:1]\n",
    "            del agent.done_history[:1]\n",
    "            \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    agent.episode_reward_history.append(episode_reward)\n",
    "    if len(agent.episode_reward_history) > 100:\n",
    "        del agent.episode_reward_history[:1]\n",
    "    agent.running_reward = np.mean(agent.episode_reward_history)\n",
    "\n",
    "    agent.episode_count += 1\n",
    "\n",
    "    if agent.running_reward > 40:  \n",
    "        print(\"Solved at episode {}!\".format(agent.episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7994d95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac49d74-fd28-435d-b795-1c989d36adc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f65a1-64de-43bf-9664-73e73267e7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c9a361-48f0-4d4f-8317-198f14898d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
